{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To use this notebook, run the following to install the dependencies. Note, the first time you use anything (library, function, etc), Julia compiles it. This means that writing `using CUDA` takes ~1m42s on my workstation the first time I run it. Be warned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.add(\"CUDA\"); Pkg.add(\"IntervalOptimisation\"); Pkg.add(\"TreeView\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First adventures\n",
    "\n",
    "Julia is like python but also like C++. It also has tons of built in math, like numpy-style array operations are in by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 2.0\n",
       " 4.0\n",
       " 6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#An example of array arithmetic\n",
    "v = [1.0, 2,3]\n",
    "v+v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Julia determined the type (Float64) from the literal's used (in this case the 1.0). It naturally has mathematical operations defined too, like vector addition. I can even do a dot product   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "dot(v,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't sexy enough. Write `v ` then `\\cdot` and press tab to autocomplete (yes latex is the way these are defined), then put `v` and you get a dot product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v ⋅ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, we have unicode, and a lot of it is plumbed into real mathematical definitions. Lets make a function, one that uses Julia's cool implicit multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(x) = 2 + 5x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Core.CodeInfo}:\n",
       " CodeInfo(\n",
       "\u001b[90m1 ─\u001b[39m %1 = 5 * x\n",
       "\u001b[90m│  \u001b[39m %2 = 2 + %1\n",
       "\u001b[90m└──\u001b[39m      return %2\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_lowered(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! I can see the code! Hmm, looks a little generic and slow. What if I put the type information in, do I see more code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Any}:\n",
       " CodeInfo(\n",
       "\u001b[90m1 ─\u001b[39m %1 = Base.mul_float(5.0, x)\u001b[36m::Float64\u001b[39m\n",
       "\u001b[90m│  \u001b[39m %2 = Base.add_float(2.0, %1)\u001b[36m::Float64\u001b[39m\n",
       "\u001b[90m└──\u001b[39m      return %2\n",
       ") => Float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_typed(f, (Float64,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its compiling to some specialised operations for floats where its tracking the types, the values, and the arguments. Do we hit bare metal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[0m.text\n",
      "\t\u001b[0m.file\t\u001b[0m\"f\"\n",
      "\t\u001b[0m.section\t\u001b[0m.rodata.cst8\u001b[0m,\u001b[0m\"aM\"\u001b[0m,\u001b[0m@progbits\u001b[0m,\u001b[33m8\u001b[39m\n",
      "\t\u001b[0m.p2align\t\u001b[33m3\u001b[39m                               \u001b[90m# -- Begin function julia_f_8971\u001b[39m\n",
      "\u001b[91m.LCPI0_0:\u001b[39m\n",
      "\t\u001b[0m.quad\t\u001b[33m0x4014000000000000\u001b[39m              \u001b[90m# double 5\u001b[39m\n",
      "\u001b[91m.LCPI0_1:\u001b[39m\n",
      "\t\u001b[0m.quad\t\u001b[33m0x4000000000000000\u001b[39m              \u001b[90m# double 2\u001b[39m\n",
      "\t\u001b[0m.text\n",
      "\t\u001b[0m.globl\t\u001b[0mjulia_f_8971\n",
      "\t\u001b[0m.p2align\t\u001b[33m4\u001b[39m\u001b[0m, \u001b[33m0x90\u001b[39m\n",
      "\t\u001b[0m.type\t\u001b[0mjulia_f_8971\u001b[0m,\u001b[0m@function\n",
      "\u001b[91mjulia_f_8971:\u001b[39m                           \u001b[90m# @julia_f_8971\u001b[39m\n",
      "\u001b[90m; ┌ @ /home/mjki2mb2/testJulia/test.ipynb:1 within `f`\u001b[39m\n",
      "\t\u001b[0m.cfi_startproc\n",
      "\u001b[90m# %bb.0:                                # %top\u001b[39m\n",
      "\t\u001b[96m\u001b[1mmovabsq\u001b[22m\u001b[39m\t\u001b[93m$.LCPI0_0\u001b[39m\u001b[0m, \u001b[0m%rax\n",
      "\u001b[90m; │┌ @ promotion.jl:389 within `*` @ float.jl:385\u001b[39m\n",
      "\t\u001b[96m\u001b[1mvmulsd\u001b[22m\u001b[39m\t\u001b[33m(\u001b[39m\u001b[0m%rax\u001b[33m)\u001b[39m\u001b[0m, \u001b[0m%xmm0\u001b[0m, \u001b[0m%xmm0\n",
      "\t\u001b[96m\u001b[1mmovabsq\u001b[22m\u001b[39m\t\u001b[93m$.LCPI0_1\u001b[39m\u001b[0m, \u001b[0m%rax\n",
      "\u001b[90m; │└\u001b[39m\n",
      "\u001b[90m; │┌ @ promotion.jl:388 within `+` @ float.jl:383\u001b[39m\n",
      "\t\u001b[96m\u001b[1mvaddsd\u001b[22m\u001b[39m\t\u001b[33m(\u001b[39m\u001b[0m%rax\u001b[33m)\u001b[39m\u001b[0m, \u001b[0m%xmm0\u001b[0m, \u001b[0m%xmm0\n",
      "\u001b[90m; │└\u001b[39m\n",
      "\t\u001b[96m\u001b[1mretq\u001b[22m\u001b[39m\n",
      "\u001b[91m.Lfunc_end0:\u001b[39m\n",
      "\t\u001b[0m.size\t\u001b[0mjulia_f_8971\u001b[0m, \u001b[0m.Lfunc_end0-julia_f_8971\n",
      "\t\u001b[0m.cfi_endproc\n",
      "\u001b[90m; └\u001b[39m\n",
      "                                        \u001b[90m# -- End function\u001b[39m\n",
      "\t\u001b[0m.section\t\u001b[0m\".note.GNU-stack\"\u001b[0m,\u001b[0m\"\"\u001b[0m,\u001b[0m@progbits\n"
     ]
    }
   ],
   "source": [
    "code_native(f, (Float64,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, assembly! All created whenever we use the function, but not before! So we always get compiled, fully optimised code. Wait, if we write abstract functions, that then get adapted to native code only when run, can we run on non-native devices?\n",
    "# CUDA\n",
    "Yes, just change the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "CuError",
     "evalue": "CUDA error: unknown error (code 999, ERROR_UNKNOWN)",
     "output_type": "error",
     "traceback": [
      "CUDA error: unknown error (code 999, ERROR_UNKNOWN)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:89\n",
      "  [2] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:97 [inlined]\n",
      "  [3] cuMemAllocAsync\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/lib/utils/call.jl:26 [inlined]\n",
      "  [4] alloc(::Type{CUDA.Mem.DeviceBuffer}, bytesize::Int64; async::Bool, stream::CuStream, pool::Nothing)\n",
      "    @ CUDA.Mem ~/.julia/packages/CUDA/DfvRa/lib/cudadrv/memory.jl:83\n",
      "  [5] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/pool.jl:41 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ./timing.jl:382 [inlined]\n",
      "  [7] actual_alloc(bytes::Int64; async::Bool, stream::CuStream)\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/src/pool.jl:39\n",
      "  [8] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/pool.jl:232 [inlined]\n",
      "  [9] macro expansion\n",
      "    @ ./timing.jl:382 [inlined]\n",
      " [10] #_alloc#170\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/pool.jl:313 [inlined]\n",
      " [11] #alloc#169\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/pool.jl:299 [inlined]\n",
      " [12] alloc\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/pool.jl:293 [inlined]\n",
      " [13] CuArray\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/array.jl:42 [inlined]\n",
      " [14] CuArray\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/array.jl:291 [inlined]\n",
      " [15] CuArray\n",
      "    @ ~/.julia/packages/CUDA/DfvRa/src/array.jl:296 [inlined]\n",
      " [16] CuArray(A::Vector{Int64})\n",
      "    @ CUDA ~/.julia/packages/CUDA/DfvRa/src/array.jl:305\n",
      " [17] top-level scope\n",
      "    @ ~/testJulia/test.ipynb:2"
     ]
    }
   ],
   "source": [
    "using CUDA\n",
    "gpu_v = CuArray([1,2,3,4,5,6,7,8]) #This array now lives on the GPU\n",
    "\n",
    "#Some generic function using the . to make .+ an elementwise addition\n",
    "#f(x) = x + 3x ⋅ x * x .+ 12 \n",
    "\n",
    "#f(gpu_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
